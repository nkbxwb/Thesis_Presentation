Automatically generated by Mendeley Desktop 1.15
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Demirtas2008,
abstract = {There has been a growing interest regarding generalized classes of$\backslash$ndistributions in statistical theory and practice because of their$\backslash$nflexibility in model formation. Multiple imputation under such distributions$\backslash$nthat span a broader area in the symmetry-kurtosis plane appears to$\backslash$nhave the potential of better capturing real incomplete data trends.$\backslash$nIn this article, we impute continuous univariate data that exhibit$\backslash$nvarying characteristics under two well-known distributions, assess$\backslash$nthe extent to which this procedure works properly, make comparisons$\backslash$nwith normal imputation models in terms of commonly accepted bias$\backslash$nand precision measures, and discuss possible generalizations to the$\backslash$nmultivariate case and to larger families of distributions.},
author = {Demirtas, Hakan and Hedeker, Donald},
doi = {10.1111/j.1467-9574.2007.00377.x},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Demirtas{\_}et{\_}al-2008-Statistica{\_}Neerlandica.pdf:pdf},
isbn = {0039-0402$\backslash$r1467-9574},
issn = {00390402},
journal = {Statistica Neerlandica},
keywords = {Kurtosis,Multiple imputation,Normality,Skewness,Symmetry},
number = {2},
pages = {193--205},
title = {{Imputing continuous data under some non-Gaussian distributions}},
volume = {62},
year = {2008}
}
@article{Rosenbaum1985,
abstract = {Matched sampling is a method for selecting units from a large reservoir of potential controls to produce a control group of modest size that is similar to a treated group with respect to the distribution of observed covariates. We illustrate the use of multivariate matching methods in an observational study of the effects of prenatal exposure to barbiturates on subsequent psychological development. A key idea is the use of the propensity score as a distinct matching variable.},
author = {Rosenbaum, Paul R and Rubin, Donald B},
doi = {10.1080/00031305.1985.10479383},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/2683903.pdf:pdf},
isbn = {00031305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bias reduction,Mahalanobis metric matching,Nearest available matching,Observational studies,Propensity scores},
number = {1},
pages = {33--38},
pmid = {5025595},
title = {{Constructing a Control Group Using Multivariate Matched Sampling Methods that Incorporate the Propensity Score}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1985.10479383},
volume = {39},
year = {1985}
}
@article{Austin2009,
abstract = {Researchers are increasingly using the standardized difference to compare the distribution of baseline covariates between treatment groups in observational studies. Standardized differences were initially developed in the context of comparing the mean of continuous variables between two groups. However, in medical research, many baseline covariates are dichotomous. In this article, we explore the utility and interpretation of the standardized difference for comparing the prevalence of dichotomous variables between two groups. We examined the relationship between the standardized difference, and the maximal difference in the prevalence of the binary variable between two groups, the relative risk relating the prevalence of the binary variable in one group compared to the prevalence in the other group, and the phi coefficient for measuring correlation between the treatment group and the binary variable. We found that a standardized difference of 10{\%} (or 0.1) is equivalent to having a phi coefficient of 0.05 (indicating negligible correlation) for the correlation between treatment group and the binary variable.$\backslash$nResearchers are increasingly using the standardized difference to compare the distribution of baseline covariates between treatment groups in observational studies. Standardized differences were initially developed in the context of comparing the mean of continuous variables between two groups. However, in medical research, many baseline covariates are dichotomous. In this article, we explore the utility and interpretation of the standardized difference for comparing the prevalence of dichotomous variables between two groups. We examined the relationship between the standardized difference, and the maximal difference in the prevalence of the binary variable between two groups, the relative risk relating the prevalence of the binary variable in one group compared to the prevalence in the other group, and the phi coefficient for measuring correlation between the treatment group and the binary variable. We found that a standardized difference of 10{\%} (or 0.1) is equivalent to having a phi coefficient of 0.05 (indicating negligible correlation) for the correlation between treatment group and the binary variable.},
author = {Austin, Peter C.},
doi = {10.1080/03610910902859574},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/03610910902859574.pdf:pdf},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {6},
pages = {1228--1234},
title = {{Using the Standardized Difference to Compare the Prevalence of a Binary Variable Between Two Groups in Observational Research}},
volume = {38},
year = {2009}
}
@article{Austin2011a,
abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.},
author = {Austin, Peter C.},
doi = {10.1080/00273171.2011.568786},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/hmbr46-399.pdf:pdf},
isbn = {0027-3171},
issn = {0027-3171},
journal = {Multivariate behavioral research},
number = {3},
pages = {399--424},
pmid = {21818162},
title = {{An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3144483{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {46},
year = {2011}
}
@book{Crowder2012,
abstract = {"Preface This book is an outgrowth of Classical Competing Risks (2001). I was very pleased to be encouraged by Rob Calver and Jim Zidek to write a second, expanded edition. Among other things it gives the opportunity to correct the many errors that crept into the first edition. This edition has been typed in Latex by my own fair hand, so the inevitable errors are now all down to me. The book is now divided into four sections but I won't go through describing them in detail here since the contents are listed on the next few pages. The book contains a variety of data tables together with R-code applied to them. For your convenience these can be found on the Web site at. Au: Please provideWeb site url. Survival analysis has its roots in death and disease among humans and animals, and much of the published literature reflects this. In this book, although inevitably including such data, I try to strike a more cheerful note with examples and applications of a less sombre nature. Some of the data included might be seen as a little unusual in the context, but the methodology of survival analysis extends to a wider field. Also, more prominence is given here to discrete time than is often the case. There are many excellent books in this area nowadays. In particular, I have learnt much fromLawless (2003), Kalbfleisch and Prentice (2002) and Cox and Oakes (1984). More specialised works, such as Cook and Lawless (2007, for Au: Add to recurrent events), Collett (2003, for medical applications), andWolstenholme refs"--},
author = {Crowder, Martin},
doi = {10.1201/b11893},
file = {:C$\backslash$:/Users/Nathan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crowder - 2012 - Multivariate Survival Analysis and Competing Risks.pdf:pdf},
isbn = {978-1-4398-7521-6},
title = {{Multivariate Survival Analysis and Competing Risks}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b11893},
volume = {20125284},
year = {2012}
}
@article{DAgostino1998a,
annote = {http://stats.stackexchange.com/questions/140761/survival-analysis-multiply-impute-5-datasets-to-average-one-propensity-score-th},
author = {D'Agostino, Ralph B.},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/D'Agostino - Propensity score methods for bias reduction in the comparison of a treatment to a non-randomized control group - 1998.pdf:pdf},
isbn = {0277-6715 (Print)$\backslash$r0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
number = {19},
pages = {2265--2281},
pmid = {9802183},
title = {{Propensity score methods for bias reduction in the comparison of a treatment to a non-randomized control group}},
volume = {17},
year = {1998}
}
@article{AZUR2011a,
annote = {Pretty basic overview of MICE.},
archivePrefix = {arXiv},
arxivId = {1106.4512},
author = {Azur, Melissa and Stuart, Elizabeth and Frangakis, Constantine and Leaf, Philip},
eprint = {1106.4512},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Azur{\_}et{\_}al-2011-International{\_}Journal{\_}of{\_}Methods{\_}in{\_}Psychiatric{\_}Research.pdf:pdf},
isbn = {0022-006X (Print)$\backslash$n0022-006X (Linking)},
issn = {1557-0657},
journal = {International Journal of Methods in Psychiatric Research},
number = {1},
pages = {1--5},
pmid = {21516187},
title = {{Multiple imputation by chained equations: what is it and how does it work?}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-79951999327{\&}partnerID=tZOtx3y1},
volume = {20},
year = {2011}
}
@book{Casella2006,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
author = {Casella, George and Fienberg, Stephen and Olkin, Ingram},
booktitle = {Design},
doi = {10.1016/j.peva.2007.06.006},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Hoff - A First Course in Bayesian Statistical Methods - 2006.pdf:pdf},
isbn = {9780387781884},
issn = {01621459},
pages = {618},
pmid = {10911016},
title = {{Springer Texts in Statistics}},
url = {http://books.google.com/books?id=9tv0taI8l6YC},
volume = {102},
year = {2006}
}
@book{Angrist2008,
author = {Angrist, Jd and Pischke, Js},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Angrist-Mostly Harmless Econometrics.pdf:pdf},
isbn = {9781400829828},
issn = {0007666X},
number = {March},
pages = {274},
pmid = {15344040},
title = {{Mostly harmless econometrics: An empiricist's companion}},
year = {2008}
}
@book{Rubin1987,
abstract = {ˆ},
author = {Rubin, Donald},
booktitle = {Harvard University},
doi = {10.1002/9780470316696},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Rubin - Multiple Imputation for Nonresponse in Surveys - 1987.pdf:pdf},
isbn = {ISBN: 047108705X :; 9780471087052; Series ISSN: 0271-6232; LCCN: 86-28935},
issn = {0162-1459},
number = {JOHN WILEY {\&} SONS},
pages = {15--19},
pmid = {13660112},
title = {{Multiple Imputation for Nonresponse in Surveys}},
year = {1987}
}
@article{Pigott2001,
abstract = {This paper reviews methods for handling missing data in a research study. Many researchers use ad hoc methods such as complete case analysis, available case analysis (pairwise deletion), or single-value imputation. Though these methods are easily implemented, they require assumptions about the data that rarely hold in practice. Model-based methods such as maximum likelihood using the EM algorithm and multiple imputation hold more promise for dealing with difficulties caused by missing data. While model-based methods require specialized computer programs and assumptions about the nature of the missing data, these methods are appropriate for a wider range of situations than the more commonly used ad hoc methods. The paper provides an illustration of the methods using data from an intervention study designed to increase students? ability to control their asthma symptoms.$\backslash$nThis paper reviews methods for handling missing data in a research study. Many researchers use ad hoc methods such as complete case analysis, available case analysis (pairwise deletion), or single-value imputation. Though these methods are easily implemented, they require assumptions about the data that rarely hold in practice. Model-based methods such as maximum likelihood using the EM algorithm and multiple imputation hold more promise for dealing with difficulties caused by missing data. While model-based methods require specialized computer programs and assumptions about the nature of the missing data, these methods are appropriate for a wider range of situations than the more commonly used ad hoc methods. The paper provides an illustration of the methods using data from an intervention study designed to increase students? ability to control their asthma symptoms.},
author = {Pigott, Therese D.},
doi = {10.1076/edre.7.4.353.8937},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/MissingDataReview.pdf:pdf},
isbn = {1380-3611},
issn = {1380-3611},
journal = {Educational Research and Evaluation},
number = {4},
pages = {353--383},
title = {{A Review of Methods for Missing Data}},
url = {http://www.tandfonline.com/doi/abs/10.1076/edre.7.4.353.8937},
volume = {7},
year = {2001}
}
@article{Frolich2015,
author = {Frolich, Markus and Huber, Martin and Wiesenfarth, Manuel},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Frolich, Huber, Wiesenfarth - The Finite Sample Performance of Semi- and Nonparametric Estimators for Treatment Effects and Policy Evalu.pdf:pdf},
journal = {IZA Discussion Paper},
keywords = {policy evaluation,simulation,treatment effects},
number = {8756},
title = {{The Finite Sample Performance of Semi- and Nonparametric Estimators for Treatment Effects and Policy Evaluation}},
year = {2015}
}
@article{Andersen1996,
abstract = {Goetghebeur and Ryan proposed a method for proportional hazards analyses$\backslash$nof competing risks failure-time data when the failure type is missing$\backslash$nfor some cases. This paper evaluates the properties of the method$\backslash$nusing data from a clinical trial in Hodgkin's disease. We generated$\backslash$nseveral patterns of missingness in the cause of death in `pseudo-studies'$\backslash$nderived from the study database. We found that the proposed method$\backslash$nprovided regression coefficients and inferences that were less biased$\backslash$nthan those from other methods over an increasing percentage of missingness$\backslash$nin the failure type when missingness is random, when it depends on$\backslash$nan important covariate, when it depends on failure type, and when$\backslash$nit depends on follow-up time. We present suggestions for study design$\backslash$nwith planned missingness in the failure type.},
author = {Andersen, J and Goetghebeur, E and Ryan, L},
doi = {10.1002/(SICI)1097-0258(19961030)15:20<2191::AID-SIM358>3.0.CO;2-D},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/ANDERSEN{\_}et{\_}al-1996-Statistics{\_}in{\_}Medicine.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in Medicine},
number = {October 1995},
pages = {2191--2201},
pmid = {8910963},
title = {{Missing cause of death information in the analysis of survival data}},
volume = {15},
year = {1996}
}
@book{Pintilie2006b,
abstract = {SMDFDFSDFS},
author = {Pintilie, Melania},
doi = {10.1002/9780470870709},
file = {:C$\backslash$:/Users/Nathan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pintilie - 2006 - Competing Risks-A Practical Perspective.pdf:pdf},
isbn = {9780470870709},
title = {{Competing Risks-A Practical Perspective}},
url = {http://doi.wiley.com/10.1002/9780470870709},
year = {2006}
}
@article{Reiter2008,
abstract = {Multiple imputation can handle missing data and disclosure limitation simultaneously. First, fill in the missing data to generate m completed datasets, then replace confidential values in each completed dataset with r imputations. I investigate how to select m and r.},
author = {Reiter, Jerome P},
doi = {10.1016/j.spl.2007.04.020},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/1-s2.0-S0167715207001654-main.pdf:pdf},
issn = {0167-7152},
journal = {Statistics {\&} Probability Letters},
keywords = {Confidentiality,Disclosure,Missing data,Multiple imputation,Synthetic data},
number = {1},
pages = {15--20},
title = {{Selecting the number of imputed datasets when using multiple imputation for missing data and disclosure limitation}},
url = {http://www.sciencedirect.com/science/article/pii/S0167715207001654},
volume = {78},
year = {2008}
}
@book{Morris1998,
author = {Morris, David and Kearsley, John and Williams, Chris},
pages = {321},
title = {{Cancer: A Comprehensive Clinical Guide}},
year = {1998}
}
@book{LittleRJAandRublin1987a,
author = {Little, Roderick and Rubin, Donald},
booktitle = {Wiley, New York.},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Little-Statistical analysis with missing data.pdf:pdf},
isbn = {3175723993},
title = {{Statistical Analysis with Missing Data}},
year = {1987}
}
@article{Zhao2014,
author = {Zhao, Yue and Herring, Amy H and Zhou, Haibo and Ali, Mirza W and Koch, Gary G},
doi = {10.1080/10543406.2013.860769.A},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/nihms-574902.pdf:pdf},
keywords = {multiple imputation,sensitivity analysis,time-to-event data},
number = {2},
pages = {229--253},
title = {{ANALYSES OF TIME-TO-EVENT DATA WITH POSSIBLY}},
volume = {24},
year = {2014}
}
@article{Marshall2009,
author = {Marshall, Andrea and Altman, Douglas G and Holder, Roger L and Royston, Patrick},
doi = {10.1186/1471-2288-9-57},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/1471-2288-9-57.pdf:pdf},
isbn = {1471-2288 (Electronic)$\backslash$r1471-2288 (Linking)},
issn = {1471-2288},
journal = {BMC medical research methodology},
pages = {57},
pmid = {19638200},
title = {{Combining estimates of interest in prognostic modelling studies after multiple imputation: current practice and guidelines.}},
volume = {9},
year = {2009}
}
@book{Cooper1992,
address = {Boston},
author = {Cooper, Geoffrey},
pages = {354},
publisher = {Jones and Bartlett Learning},
title = {{Elements of Human Cancer}},
year = {1992}
}
@article{Freedman2008a,
abstract = {Regressions can be weighted by propensity scores in order to reduce bias. However, weighting is likely to increase random error in the estimates, and to bias the estimated standard errors downward, even when selection mechanisms are well understood. Moreover, in some cases, weighting will increase the bias in estimated causal parameters. If investigators have a good causal model, it seems better just to fit the model without weights. If the causal model is improperly specified, there can be significant problems in retrieving the situation by weighting, although weighting may help under some circumstances.},
author = {Freedman, David a and Berk, Richard a},
doi = {10.1177/0193841X08317586},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/weight.pdf:pdf},
isbn = {0193-841X (Print)},
issn = {0193-841X},
journal = {Evaluation review},
keywords = {causation,experiments,models,observational studies,propensity scores,regression,selection},
number = {4},
pages = {392--409},
pmid = {18591709},
title = {{Weighting regressions by propensity scores.}},
volume = {32},
year = {2008}
}
@article{Cox1972,
abstract = {The abalysis of censored failure times is considered . assumed on each individual are available values of on or more explanatory variables. the hazard function is taken to be function of the explanatory variables and unknown function of time. a conditional likelihood is obtained lading to ibferense about the unknown regression coffcients.},
author = {Cox, D.R.},
doi = {10.2307/2985181},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Cox - Regression Models and Life-Tables - 1972.pdf:pdf},
isbn = {00359246},
issn = {00359246},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {AGE SPECIFIC RATE,CENSORED DATA,HAZARD FUNCTION,LIFE TABLE,PRODUCT LIMIT ESTIMATE,REGRESSION,RELAYABILITY,THEORY: ACCELERATED LIFE TEST,TWO SAMPLE RANK TEST: MEDICAL APPLICATION,asymptotic theory,conditional inference},
number = {2},
pages = {187--220},
pmid = {2985181},
title = {{Regression Models and Life-Tables}},
url = {http://links.jstor.org/sici?sici=0035-9246(1972)34:2<187:RMAL>2.0.CO;2-6$\backslash$nhttp://www.jstor.org},
volume = {34},
year = {1972}
}
@book{Boyd2010,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Boyd-Convex Optimization.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
url = {https://web.stanford.edu/{~}boyd/cvxbook/bv{\_}cvxbook.pdf},
volume = {25},
year = {2010}
}
@article{Honaker2015,
author = {Honaker, James and King, Gary and Blackwell, Matthew},
doi = {10.1.1.149.9611},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/v45i07.pdf:pdf},
isbn = {1548-7660},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {bootstrap,missing data,multiple imputation,r},
number = {7},
pages = {1--54},
title = {{AMELIA II : A Program for Missing Data}},
url = {http://gking.harvard.edu/amelia/},
volume = {45},
year = {2011}
}
@article{Li2013,
abstract = {<p>Propensity score (PS) matching is widely used for studying treatment effects in observational studies. This article proposes the method of matching weights (MWs) as an analog to one-to-one pair matching without replacement on the PS with a caliper. Compared with pair matching, the proposed method offers more efficient estimation, more accurate variance calculation, better balance, and simpler asymptotic analysis. A statistical test for the misspecification of the PS model is proposed for balance checking purposes. An augmented version of the MW estimator is developed that has the double robust property, that is, the estimator is consistent, if either the outcome model or the PS model is correct. The proposed method is studied in simulations and illustrated through a real data example.</p>},
author = {Li, Liang and Greene, Tom},
doi = {10.1515/ijb-2012-0030},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/ijb-2012-0030.pdf:pdf},
issn = {1557-4679},
journal = {The International Journal of Biostatistics},
keywords = {anderson cancer center,causal inference,corresponding author,department of biostatistics,division of epidemiology,double robust,e-mail,houston,inverse probability weighting,liang li,lli15,matching weight,mdanderson,mirror histogram,org,salt lake city,texas,tom greene,university of texas md,university of utah,usa,utah},
number = {2},
pages = {215--234},
title = {{A Weighting Analogue to Pair Matching in Propensity Score Analysis}},
url = {http://www.degruyter.com/view/j/ijb.2013.9.issue-2/ijb-2012-0030/ijb-2012-0030.xml},
volume = {9},
year = {2013}
}
@book{Collett,
author = {Collett, David},
isbn = {9781489931153},
title = {{Modelling Survival Data in Medical Research}}
}
@misc{Guo2010,
abstract = {Propensity score analysis is a relatively new and innovative class of statical methods that has proven useful treatment effects when using nonexperimental or observational data. Specifically; propensity score analysis offers an approach to program evaluation when randomized clinical trials are infeasible or unethical, or when researchers need to assess treatment effects from survey data, census data, administrative data, or other types of data " collected though the observations of systems as they operate in normal practice without any interventions implemnted by randomized assignment rules" (Rubin, 1997, p.757)},
author = {Guo, Shenyang and Fraser, Mark W.},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/[Shenyang{\_}Y.{\_}Guo,{\_}Mark{\_}W.{\_}Fraser]{\_}Propensity{\_}Score(BookZZ.org).pdf:pdf},
isbn = {978-1-4129-5356-6},
keywords = {Social Scienses,observational data,propensity score,randomized},
pages = {370},
title = {{Propensity score analysis. Statistical methods and application}},
year = {2010}
}
@article{Austin2014,
author = {Austin, Peter C.},
doi = {10.1002/sim.5984},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Austin-2014-Statistics{\_}in{\_}Medicine.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {confounding,event history analysis,inverse probability of treatment,marginal effects,observational study,propensity score,propensity score matching,survival analysis,weighting},
number = {7},
pages = {1242--1258},
title = {{The use of propensity score methods with survival or time-to-event outcomes: reporting measures of effect similar to those used in randomized experiments}},
url = {http://doi.wiley.com/10.1002/sim.5984},
volume = {33},
year = {2014}
}
@article{Bishop1995,
abstract = {This book provides a solid statistical foundation for neural networks from a pattern recognition perspective. The focus is on the types of neural nets that are most widely used in practical applications, such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many different types of neural networks, Bishop thoroughly covers topics such as density estimation, error functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are organized well and all mathematical foundations are explained before being applied to neural networks. The text is suitable for a graduate or advanced undergraduate level course on neural networks or for practitioners interested in applying neural networks to real-world problems. The reader is assumed to have the level of math knowledge necessary for an undergraduate science degree. This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
author = {Bishop, C M},
doi = {10.2307/2965437},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Bishop{\_}Neural Networks for Pattern Recognition.pdf:pdf},
isbn = {0198538642},
issn = {01621459},
journal = {Journal of the American Statistical Association},
pages = {482},
title = {{Neural Networks for Pattern Recognition}},
volume = {92},
year = {1995}
}
@article{Royston2004a,
author = {Royston, Patrick},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/sjart{\_}st0067.pdf:pdf},
journal = {The Stata Journal},
keywords = {micombine,mijoin,misplit,missing at random,missing data,multiple imputation,multivariate imputation,mvis,regression modeling,st0067,uvis},
number = {3},
pages = {227--241},
title = {{Multiple imputation of missing values}},
year = {2004}
}
@article{Busso2014,
author = {Busso, M and DiNardo, J and McCrary, J},
doi = {10.1162/REST},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/BDM2011.pdf:pdf},
issn = {0034-6535},
journal = {Review of Economics and Statistics},
keywords = {pq{\_}tech{\_}Nearest{\_}Neighbor},
number = {2004},
pages = {"Just Accepted" section},
title = {{New Evidence on the Finite Sample Properties of Propensity Score Reweighting and Matching Estimators}},
volume = {(not yet a},
year = {2014}
}
@article{Brand2003,
abstract = {This paper outlines a strategy to validate multiple imputation methods. Rubin's criteria for proper multiple imputation are the point of departure. We describe a simulation method that yields insight into various aspects of bias and efficiency of the imputation process. We propose a new method for creating incomplete data under a general Missing At Random {\{}(MAR){\}} mechanism. Software implementing the validation strategy is available as a {\{}SAS/IML{\}} module. The method is applied to investigate the behavior of polytomous regression imputation for categorical data.},
author = {Brand, Jaap P L and {Van Buuren}, Stef and Groothuis-Oudshoorn, Karin and Gelsema, Edzard S.},
doi = {10.1111/1467-9574.00219},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Brand{\_}et{\_}al-2003-Statistica{\_}Neerlandica.pdf:pdf},
issn = {00390402},
journal = {Statistica Neerlandica},
keywords = {Missing data mechanism,Multiple imputation,Proper imputation,Simulation},
number = {1},
pages = {36--45},
pmid = {38},
title = {{A toolkit in SAS for the evaluation of multiple imputation methods}},
volume = {57},
year = {2003}
}
@article{Gelman2001a,
author = {Gelman, Andrew and Raghunathan, Te},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/arnold2.pdf:pdf},
journal = {Statistical Science},
pages = {268--269},
title = {{Using conditional distributions for missing-data imputation}},
volume = {15},
year = {2001}
}
@article{Crump2006,
author = {Crump, Richard K and Hotz, V Joseph and Imbens, Guido W and Crump, Richard},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/t0330.pdf:pdf},
journal = {NATIONAL BUREAU OF ECONOMIC RESEARCH},
title = {{Technical Working Paper Series Moving the Goalposts : Addressing Limited Overlap in the Estimation}},
year = {2006}
}
@article{Gayat2012,
annote = {Doesn't talk about weighting},
author = {Gayat, Etienne and Resche-Rigon, Matthieu and Mary, Jean-Yves and Porcher, Rapha{\"{e}}l},
doi = {10.1002/pst.537},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Gayat{\_}et{\_}al-2012-Pharmaceutical{\_}Statistics.pdf:pdf},
issn = {15391604},
journal = {Pharmaceutical Statistics},
keywords = {bias,propensity score,simulation,survival,treatment effect},
number = {3},
pages = {222--229},
title = {{Propensity score applied to survival data analysis through proportional hazards models: a Monte Carlo study}},
url = {http://doi.wiley.com/10.1002/pst.537},
volume = {11},
year = {2012}
}
@article{Kaplan1958,
abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occur- rence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has ? o <tjI <t.2' < . .<tN'. Then P(t) =11r [(N-r)/(N-r+1)], where r assumes those values for which tr'<t and for which tr/ measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases re- duces to the usual binomial estimate, namely, the observed proportion of survivors},
author = {Kaplan, E L and Meier, Paul},
doi = {10.2307/2281868},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/2281868.pdf:pdf},
isbn = {01621459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {282},
pages = {457--481},
pmid = {252},
title = {{Nonparametric Estimation from Incomplete Observations}},
volume = {53},
year = {1958}
}
@article{norm2015,
author = {Novo, Alvaro A and Schafer, Joseph L},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/norm (1).pdf:pdf},
journal = {CRAN},
title = {{Package ‘ norm ’}},
year = {2015}
}
@article{Su2011,
abstract = {Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: exible choice of predictors, models, and transformations for chained imputation models; binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data in one and two dimensions. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be incorporated into the software of the others.},
author = {Su, Yu-Sung and Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/v45i02.pdf:pdf},
isbn = {1548-7660},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {R,chained eq,chained equations,mi,model diagnostics,multiple imputation,weakly informative,weakly informative prior},
number = {2},
pages = {1--31},
title = {{Multiple Imputation with Diagnostics (mi) in R: Opening Windows into the Black Box}},
volume = {45},
year = {2011}
}
@article{Lee2010a,
abstract = {Statistical analysis in epidemiologic studies is often hindered by missing data, and multiple imputation is increasingly being used to handle this problem. In a simulation study, the authors compared 2 methods for imputation that are widely available in standard software: fully conditional specification (FCS) or "chained equations" and multivariate normal imputation (MVNI). The authors created data sets of 1,000 observations to simulate a cohort study, and missing data were induced under 3 missing-data mechanisms. Imputations were performed using FCS (Royston's "ice") and MVNI (Schafer's NORM) in Stata (Stata Corporation, College Station, Texas), with transformations or prediction matching being used to manage nonnormality in the continuous variables. Inferences for a set of regression parameters were compared between these approaches and a complete-case analysis. As expected, both FCS and MVNI were generally less biased than complete-case analysis, and both produced similar results despite the presence of binary and ordinal variables that clearly did not follow a normal distribution. Ignoring skewness in a continuous covariate led to large biases and poor coverage for the corresponding regression parameter under both approaches, although inferences for other parameters were largely unaffected. These results provide reassurance that similar results can be expected from FCS and MVNI in a standard regression analysis involving variously scaled variables.},
author = {Lee, Katherine J. and Carlin, John B.},
doi = {10.1093/aje/kwp425},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/624.full.pdf:pdf},
isbn = {1476-6256 (Electronic)$\backslash$n0002-9262 (Linking)},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {Data interpretation,Epidemiologic methods,Imputation,Incomplete data,Missing data,Simulations,Statistical},
number = {5},
pages = {624--632},
pmid = {20106935},
title = {{Multiple imputation for missing data: Fully conditional specification versus multivariate normal imputation}},
volume = {171},
year = {2010}
}
@article{White2011a,
abstract = {Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments.},
author = {White, Ian and Royston, Patrick and Wood, Angela M.},
doi = {10.1002/sim.4067},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/White{\_}et{\_}al-2011-Statistics{\_}in{\_}Medicine.pdf:pdf},
isbn = {1097-0258 (Electronic)$\backslash$n0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Fully conditional specification,Missing data,Multiple imputation},
number = {4},
pages = {377--399},
pmid = {21225900},
title = {{Multiple imputation using chained equations: Issues and guidance for practice}},
volume = {30},
year = {2011}
}
@article{Stuart2009a,
abstract = {Multiple imputation is an effective method for dealing with missing data, and it is becoming increasingly common in many fields. However, the method is still relatively rarely used in epidemiology, perhaps in part because relatively few studies have looked at practical questions about how to implement multiple imputation in large data sets used for diverse purposes. This paper addresses this gap by focusing on the practicalities and diagnostics for multiple imputation in large data sets. It primarily discusses the method of multiple imputation by chained equations, which iterates through the data, imputing one variable at a time conditional on the others. Illustrative data were derived from 9,186 youths participating in the national evaluation of the Community Mental Health Services for Children and Their Families Program, a US federally funded program designed to develop and enhance community-based systems of care to meet the needs of children with serious emotional disturbances and their families. Multiple imputation was used to ensure that data analysis samples reflect the full population of youth participating in this program. This case study provides an illustration to assist researchers in implementing multiple imputation in their own data.},
author = {Stuart, Elizabeth and Azur, Melissa and Frangakis, Constantine and Leaf, Philip},
doi = {10.1093/aje/kwp026},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/1133.full.pdf:pdf},
isbn = {1476-6256 (Electronic)$\backslash$n0002-9262 (Linking)},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {Mental health services,Missing at random,Missing data,Multiple imputation},
number = {9},
pages = {1133--1139},
pmid = {19318618},
title = {{Multiple imputation with large data sets: A case study of the children's mental health initiative}},
volume = {169},
year = {2009}
}
@article{Austin2011a,
annote = {Standardized differences will be a useful tool},
author = {Austin, Peter C.},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/hmbr46{\_}119.pdf:pdf},
isbn = {0027-3171 (Print)$\backslash$n0027-3171 (Linking)},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
number = {1},
pages = {119--151},
pmid = {22287812},
title = {{A Tutorial and Case Study in Propensity Score Analysis: An Application to Estimating the Effect of In-Hospital Smoking Cessation Counseling on Mortality}},
volume = {46},
year = {2011}
}
@article{Casella2001,
abstract = {This solutions manual contains solutions for all odd numbered problems plus a large number of solutions for even numbered problems. Of the 624 exercises in Statistical Inference, Second Edition, this manual gives solutions for 484 (78{\%}) of them. There is an obtuse pattern as to which solutions were included in this manual. We assembled all of the solutions that we had from the first edition, and filled in so that all odd-numbered problems were done. In the passage from the first to the second edition, problems were shuffled with no attention paid to numbering (hence no attention paid to minimize the new effort), but rather we tried to put the problems in logical order. A major change from the first edition is the use of the computer, both symbolically through Mathematicatm and numerically using R. Some solutions are given as code in either of these lan- guages. Mathematicatm can be purchased from Wolfram Research, and R is a free download from http://www.r-project.org/.},
author = {Casella, George and Berger, Roger L. and Santana, Damaris},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Casella{\_}berger{\_}solutions.pdf:pdf},
pages = {195},
title = {{Solutions Manual for Statistical Inference, Second Edition}},
year = {2001}
}
@article{Long2012,
author = {Long, Qi and Hsu, Chiu-Hsieh and Li, Yisheng},
doi = {10.5705/ss.2010.069},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/nihms-296043.pdf:pdf},
isbn = {1017-0405 (Print)$\backslash$n1017-0405 (Linking)},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {doubly robust,imputation,missing at random,multiple imputation,nearest neighbor,nonparametric,sensitivity analysis},
number = {1},
pages = {1--22},
pmid = {22347786},
title = {{Doubly robust nonparametric multiple imputation for ignorable missing data}},
volume = {22},
year = {2012}
}
@article{Florian2015,
author = {Florian, Author and Meinfelder, Maintainer Florian},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Florian, Meinfelder - BaBooN Bayesian Bootstrap Predictive Mean Matching - Multiple and Single Imputation for Discrete Data - 2015.pdf:pdf},
title = {{BaBooN: Bayesian Bootstrap Predictive Mean Matching - Multiple and Single Imputation for Discrete Data}},
url = {http://cran.r-project.org/package=BaBooN},
year = {2015}
}
@book{VanBuuren2012,
abstract = {"Preface We are surrounded by missing data. Problems created by missing data in statistical analysis have long been swept under the carpet. These times are now slowly coming to an end. The array of techniques to deal with missing data has expanded considerably during the last decennia. This book is about one such method: multiple imputation. Multiple imputation is one of the great ideas in statistical science. The technique is simple, elegant and powerful. It is simple because it flls the holes in the data with plausible values. It is elegant because the uncertainty about the unknown data is coded in the data itself. And it is powerful because it can solve 'other' problems that are actually missing data problems in disguise. Over the last 20 years, I have applied multiple imputation in a wide variety of projects. I believe the time is ripe for multiple imputation to enter mainstream statistics. Computers and software are now potent enough to do the required calculations with little e ort. What is still missing is a book that explains the basic ideas, and that shows how these ideas can be put to practice. My hope is that this book can ll this gap. The text assumes familiarity with basic statistical concepts and multivariate methods. The book is intended for two audiences: - (bio)statisticians, epidemiologists and methodologists in the social and health sciences; - substantive researchers who do not call themselves statisticians, but who possess the necessary skills to understand the principles and to follow the recipes. In writing this text, I have tried to avoid mathematical and technical details as far as possible. Formula's are accompanied by a verbal statement that explains the formula in layman terms"--},
author = {van Buuren, Stef},
booktitle = {Chapman {\&} Hall/CRC interdisciplinary statistics series},
doi = {10.1201/b11826},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Van Buuren - Flexible Imputation of Missing Data - 2012.pdf:pdf},
isbn = {9781439868249 (hardback)},
keywords = {Missing observations (Statistics),Multiple imputation (Statistics),Multivariate analysis.},
pages = {xxv, 316 p.},
pmid = {17176045},
title = {{Flexible imputation of missing data}},
year = {2012}
}
@article{Mitra2012,
abstract = {In many observational studies, analysts estimate treatment effects using propensity scores, e.g. by matching or sub-classifying on the scores. When some values of the covariates are missing, analysts can use multiple imputation to fill in the missing data, estimate propensity scores based on the m completed datasets, and use the propensity scores to estimate treatment effects. We compare two approaches to implement this process. In the first, the analyst estimates the treatment effect using propensity score matching within each completed data set, and averages the m treatment effect estimates. In the second approach, the analyst averages the m propensity scores for each record across the completed datasets, and performs propensity score matching with these averaged scores to estimate the treatment effect. We compare properties of both methods via simulation studies using artificial and real data. The simulations suggest that the second method has greater potential to produce substantial bias reductions than the first, particularly when the missing values are predictive of treatment assignment.},
annote = {I did a google search for this

I am going to need to write out what is happening to make sense of it},
author = {Mitra, R. and Reiter, J. P.},
doi = {10.1177/0962280212445945},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Stat Methods Med Res-2012-Mitra-0962280212445945.pdf:pdf},
issn = {0962-2802},
journal = {Statistical Methods in Medical Research},
keywords = {missing data,multiple imputation,observational studies,propensity score},
pages = {1--17},
pmid = {22687877},
title = {{A comparison of two methods of estimating propensity scores after multiple imputation}},
year = {2012}
}
@article{Freedman2008,
abstract = {Regressions can be weighted by propensity scores in order to reduce bias. However, weighting is likely to increase random error in the estimates, and to bias the estimated standard errors downward, even when selection mechanisms are well understood. Moreover, in some cases, weighting will increase the bias in estimated causal parameters. If investigators have a good causal model, it seems better just to fit the model without weights. If the causal model is improperly specified, there can be significant problems in retrieving the situation by weighting, although weighting may help under some circumstances.},
author = {Freedman, David a and Berk, Richard a},
doi = {10.1177/0193841X08317586},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/weight.pdf:pdf},
isbn = {0193-841X (Print)},
issn = {0193-841X},
journal = {Evaluation review},
keywords = {causation,experiments,models,observational studies,propensity scores,regres-,scores,selection,sion,weighting regressions by propensity},
number = {4},
pages = {392--409},
pmid = {18591709},
title = {{Weighting regressions by propensity scores.}},
volume = {32},
year = {2008}
}
@article{Dewanji1992,
abstract = {The modified log rank test for competing risks with missing failure type suggested by Goetghebeur {\&} Ryan (1990) is derived from a partial likelihood which leaves out some information. This extra information is incorporated to give a new partial likelihood and hence a new test statistic.},
author = {Dewanji, Anup},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/855.full.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {855--857},
title = {{A note on a test for competing risks with missing failure type}},
volume = {79},
year = {1992}
}
@book{Therneau2002b,
abstract = {This is a book for statistical practitioners, particularly those who design and analyze studies for survival and event history data. Its goal is to extend the toolkit beyond the basic triad provided by most statistical packages: the Kaplan-Meier estimator, log-rank test, and Cox regression model.},
author = {Therneau, Terry and Grambsch, Patricia},
booktitle = {Technometrics},
doi = {10.1198/tech.2002.s656},
file = {:C$\backslash$:/Users/Nathan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Therneau, Grambsch - 2002 - Modeling Survival Data Extending the Cox Model.pdf:pdf},
isbn = {0387987843},
issn = {0040-1706},
number = {1},
pages = {85--86},
pmid = {43993527},
title = {{Modeling Survival Data: Extending the Cox Model}},
volume = {44},
year = {2002}
}
@article{White2009,
abstract = {Although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. Here, we propose a method for calculating power and sample size for studies using current status data. This method is based on a Weibull survival model for a two-group comparison. The Weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. We consider exponential, Weibull and uniformly distributed censoring distributions. We base our power calculations on a parametric approach with the Wald test because it is easy for medical investigators to conceptualize and specify the required input variables. As expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. Our simulation results demonstrate the merits of these proposed power calculations.},
author = {White, Ian and Royston, Patrick},
doi = {10.1002/sim},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/White, Royston - Imputing missing covariate values for the Cox model - 2009.pdf:pdf},
isbn = {2007090091480},
issn = {02776715},
journal = {Statistics in medicine},
keywords = {missing covariates,missing data,multiple imputation,proportional hazards model},
number = {15},
pages = {1982--1998},
pmid = {19455509},
title = {{Imputing missing covariate values for the Cox model}},
volume = {28},
year = {2009}
}
@article{Kropko2014,
abstract = {We consider the relative performance of two common approaches to multiple imputation (MI): joint MI, in which the data are modeled as a sample from a joint distribution; and conditional MI, in which each variable is modeled conditionally on all the others. Implementations of joint MI are typically restricted in two ways: first, the joint distribution of the data is assumed to be multivariate normal, and second, in order to use the multivariate normal distribution, categories of discrete variables are assumed to be probabilistically constructed from continuous values. We use simulations to examine the implications of these assumptions. For each approach, we assess (1) the accuracy of the imputed values, and (2) the accuracy of coefficients and fitted values from a model fit to completed datasets. These simulations consider continuous, binary, ordinal, and unordered-categorical variables. One set of simulations uses multivariate normal data and one set uses data from the 2008 American National Election Study. We implement a less restrictive approach than is typical when evaluating methods using simulations in the missing data literature: in each case, missing values are generated by carefully following the conditions necessary for missingness to be “missing at random” (MAR).We find that in these situations conditional MI is more accurate than joint multivariate normal MI whenever the data include categorical variables.},
annote = {We find that in these situations conditional MI is more accurate than joint MVN MI whenever the data include categorical variables.

In general, we see that there are two kinds of results. Either conditional MI outperforms jointMVN MI and the other competitors by a fair margin, or there is very little difference between joint MVN MI and conditional MI. In no case does joint MVN MI clearly outperform conditional MI.},
author = {Kropko, Jonathan and Goodrich, Ben and Gelman, Andrew and Hill, Jennifer},
doi = {10.1093/pan/mpu007},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Political Analysis-2014-Kropko-497-519.pdf:pdf},
issn = {1047-1987},
journal = {Political Analysis},
pages = {497--519},
title = {{Multiple Imputation for Continuous and Categorical Data: Comparing Joint Multivariate Normal and Conditional Approaches}},
url = {http://pan.oxfordjournals.org/cgi/doi/10.1093/pan/mpu007},
year = {2014}
}
@article{Wood2005,
abstract = {BACKGROUND: Longitudinal studies almost always have some individuals with missing outcomes. Inappropriate handling of the missing data in the analysis can result in misleading conclusions. Here we review a wide range of methods to handle missing outcomes in single and repeated measures data and discuss which methods are most appropriate. METHODS: Using data from a randomized controlled trial to compare two interventions for increasing physical activity, we compare complete-case analysis; ad hoc imputation techniques such as last observation carried forward and worst-case; model-based imputation; longitudinal models with random effects; and recently proposed joint models for repeated measures data and non-ignorable dropout. RESULTS: Estimated intervention effects from ad hoc imputation methods vary widely. Standard multiple imputation and longitudinal modelling agree closely, as they should. Modifying the modelling method to allow for non-ignorable dropout had little effect on estimated intervention effects, but imputing using a common imputation model in both groups gave more conservative results. CONCLUSIONS: Results from ad hoc imputation methods should be avoided in favour of methods with more plausible assumptions although they may be computationally more complex. Although standard multiple imputation methods and longitudinal modelling methods are equivalent for estimating the treatment effect, the two approaches suggest different ways of relaxing the assumptions, and the choice between them depends on contextual knowledge.},
author = {Wood, Angela M. and White, Ian and Hillsdon, Melvyn and Carpenter, James},
doi = {10.1093/ije/dyh297},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/89.full.pdf:pdf},
isbn = {03005771},
issn = {03005771},
journal = {International Journal of Epidemiology},
keywords = {Dropout,Imputation,Last observation carried forward,Longitudinal data,Missing data,Non-ignorable,Random effects},
number = {1},
pages = {89--99},
pmid = {15333619},
title = {{Comparison of imputation and modelling methods in the analysis of a physical activity trial with missing outcomes}},
volume = {34},
year = {2005}
}
@article{Wang1998,
abstract = {We consider the asymptotic behaviour of various parametric multiple imputation procedures which include but are not restricted to the `proper' imputation procedures proposed by Rubin (1978). The asymptotic variance structure of the resulting estimators is provided. This result is used to compare the relative efficiencies of different imputation procedures. It also provides a basis to understand the behaviour of two Monte Carlo iterative estimators, stochastic EM (Celeux {\&} Diebolt, 1985; Wei {\&} Tanner, 1990) and simulated EM (Ruud, 1991). We further develop properties of these estimators when they stop at iteration K with imputation size m. An application to a measurement error problem is used to illustrate the results. CR - Copyright {\&}{\#}169; 1998 Biometrika Trust},
author = {Wang, Naisyin and Robins, James M},
doi = {10.2307/2337494},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/935.full.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
number = {4},
pages = {935--948},
pmid = {185},
title = {{Large-Sample Theory for Parametric Multiple Imputation Procedures}},
url = {http://www.jstor.org/stable/2337494},
volume = {85},
year = {1998}
}
@article{Austin2014a,
author = {Austin, Peter C. and Schuster, T.},
doi = {10.1177/0962280213519716},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Stat Methods Med Res-2014-Austin-0962280213519716.pdf:pdf},
issn = {0962-2802},
journal = {Statistical Methods in Medical Research},
keywords = {inverse probability of treatment,monte carlo simulations,observational study,propensity score,survival analysis,time-to-event outcomes,weighting},
pages = {1--24},
title = {{The performance of different propensity score methods for estimating absolute effects of treatments on survival outcomes: A simulation study}},
url = {http://smm.sagepub.com/cgi/doi/10.1177/0962280213519716},
year = {2014}
}
@article{King2015,
abstract = {Working paper},
author = {King, Gary and Nielsen, Richard},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/psnot.pdf:pdf},
journal = {Working paper},
title = {{Why Propensity Scores Should Not Be Used for Matching}},
year = {2015}
}
@article{Liu2014a,
abstract = {Iterative imputation, in which variables are imputed one at a time each given a model predicting from all the others, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modeling problem with relatively simple univariate regressions. In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties. More precisely, when the conditional models are compatible (defined in the text), we give a set of sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but are valid, we show that the combined imputation estimator is consistent.},
archivePrefix = {arXiv},
arxivId = {1012.2902},
author = {Liu, Jingchen and Gelman, Andrew and Hill, Jennifer and Su, Yu-Sung and Kropko, Jonathan},
doi = {10.1093/biomet/ast044},
eprint = {1012.2902},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/mi{\_}theory9.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Chained equation,Convergence,Iterative imputation,Markov chain},
number = {1},
pages = {155--173},
title = {{On the stationary distribution of iterative imputations}},
volume = {101},
year = {2014}
}
@article{Kenward2007a,
author = {Kenward, Michael and Carpenter, James},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/199.full.pdf:pdf},
isbn = {0962-2802 (Print)$\backslash$r0962-2802 (Linking)},
journal = {Stat Methods Med Res},
keywords = {Models, Statistical},
number = {3},
pages = {199--218},
pmid = {17621468},
title = {{Multiple imputation: current perspectives}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17621468$\backslash$nhttp://smm.sagepub.com/content/16/3/199.full.pdf},
volume = {16},
year = {2007}
}
@article{Brookhart2008,
annote = {{\&}quot;The results suggest that variables that are unrelated to the exposure but related to the outcome should always be included in a PS model. The{\&}quot;},
author = {Brookhart, M. Alan and Schneeweiss, Sebastian and Rothman, Kenneth J. and Glynn, Robert J. and Avorn, Jerry and Til, St{\"{u}}rmer},
doi = {10.1016/j.drugalcdep.2008.02.002.A},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Brookhart et al. - Variable selection for propensity score models - 2008.pdf:pdf},
issn = {08966273},
keywords = {confounding,propensity scores,simulation,stepwise regression,subset selection,variable selection},
number = {10},
pages = {1203--1214},
title = {{Variable selection for propensity score models}},
volume = {15},
year = {2008}
}
@article{VanBuuren2007,
abstract = {The goal of multiple imputation is to provide valid inferences for statistical estimates from incomplete data. To achieve that goal, imputed values should preserve the structure in the data, as well as the uncertainty about this structure, and include any knowledge about the process that generated the missing data. Two approaches for imputing multivariate data exist: joint modeling (JM) and fully conditional specification (FCS). JM is based on parametric statistical theory, and leads to imputation procedures whose statistical properties are known. JM is theoretically sound, but the joint model may lack flexibility needed to represent typical data features, potentially leading to bias. FCS is a semi-parametric and flexible alternative that specifies the multivariate model by a series of conditional models, one for each incomplete variable. FCS provides tremendous flexibility and is easy to apply, but its statistical properties are difficult to establish. Simulation work shows that FCS behaves very well in the cases studied. The present paper reviews and compares the approaches. JM and FCS were applied to pubertal development data of 3801 Dutch girls that had missing data on menarche (two categories), breast development (five categories) and pubic hair development (six stages). Imputations for these data were created under two models: a multivariate normal model with rounding and a conditionally specified discrete model. The JM approach introduced biases in the reference curves, whereas FCS did not. The paper concludes that FCS is a useful and easily applied flexible alternative to JM when no convenient and realistic joint distribution can be specified.},
author = {van Buuren, Stef},
doi = {10.1177/0962280206074463},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/219.full.pdf:pdf},
isbn = {0962-2802 (Print)$\backslash$r0962-2802 (Linking)},
issn = {0962-2802},
journal = {Statistical methods in medical research},
number = {3},
pages = {219--242},
pmid = {17621469},
title = {{Multiple imputation of discrete and continuous data by fully conditional specification.}},
volume = {16},
year = {2007}
}
@article{Barnard1999,
abstract = {An appealing feature of multiple imputation is the simplicity of the rules for combining the multiple complete-data inferences into a final inference, the repeated-imputation inference (Rubin, 1987). This inference is based on a t distribution and is derived from a Bayesian paradigm under the assumption that the complete-data degrees of freedom, v(com), are infinite, but the number of imputations, m, is finite. When v(com) is small and there is only a modest proportion of missing data, the calculated repeated-imputation degrees of freedom, v(m), for the t reference distribution can be much larger than v(com), which is clearly inappropriate. Following the Bayesian paradigm, we derive an adjusted degrees of freedom, (v) over tilde(m), with the following three properties: for fixed m and estimated fraction of missing information, (v) over tilde(m) monotonically increases in v(com); (v) over tilde(m) is always less than or equal to v(com); and (v) over tilde(m) equals v(m) when v(com) is infinite. A small simulation study demonstrates the superior frequentist performance when using (v) over tilde(m) rather than v(m).},
author = {Barnard, J and Rubin, Donald},
doi = {DOI 10.1093/biomet/86.4.948},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/2673599.pdf:pdf},
isbn = {0006-3444},
issn = {0006-3444},
journal = {Biometrika},
keywords = {bayesian inference,fraction of missing information,missing at random,missing data mechanism,repeated imputation},
number = {4},
pages = {948--955},
title = {{Small-sample degrees of freedom with multiple imputation}},
url = {<Go to ISI>://000084833000018},
volume = {86},
year = {1999}
}
@article{Burgette2014,
abstract = {The Toolkit for Weighting and Analysis of Nonequivalent Groups, twang, was designed to make causal estimates in the binary treatment setting. In twang versions 1.3 and later, we have extended this software package to handle more than two treatment conditions through the mnps function, which stands for multinomial propensity scores. McCaffrey et al. (2013) describe the methodology behind the mnps function; the purpose of this document is to describe the syntax and features related to the implementation in twang},
author = {Burgette, Lane F and Griffin, Beth Ann and McCaffrey, Daniel F},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/mnps.pdf:pdf},
number = {1},
pages = {1--16},
title = {{Propensity scores for multiple treatments : A tutorial for the mnps function in the twang package}},
volume = {015697},
year = {2014}
}
@article{Posner2012a,
annote = {This looks unfinished. Might want to look at 


Comparing Standard Regression, Propensity Score Matching, and Instrumental Variables Methods for Determining the Effectiveness of Mammography in Older Women.  Michael A. Posner, Arlene Ash, Michael Shwartz, Karen Freund, Mark Moskowitz. Health Services and Outcomes Research Methodology, 2001; 2:279-290.},
author = {Posner, Ma and Ash, Arlene},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/posner.pdf:pdf},
journal = {Unpublished working paper, Columbia {\ldots}},
title = {{Comparing weighting methods in propensity score analysis}},
url = {http://www.stat.columbia.edu/{~}gelman/stuff{\_}for{\_}blog/posner.pdf},
year = {2012}
}
@article{Goetghebeur1990,
abstract = {We propose a modified log rank test for the analysis of competing risks survival data, when failure type is missing for some individuals. The proposed test reduces to a standard log rank test when all failure types are known. The test arises from a partial likelihood, constructed under semiparametric assumptions on the relationship between cause-specific hazards.},
author = {Goetghebeur, E and Ryan, Louise},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/207.full.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {1},
pages = {207--211},
title = {{A modified log rank test for competing risks with missing failure type}},
url = {http://biomet.oxfordjournals.org/content/77/1/207.short},
volume = {77},
year = {1990}
}
@article{Andreis2012,
author = {Andreis, Federico and Ferrari, Pier Alda},
doi = {10.1285/i20705948v5n3p431},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/12336-14155-1-PB.pdf:pdf},
journal = {Electronic Journal of Applied Statistical Analysis},
keywords = {dichotomous response,imputation methods,mcmc,simulation study},
number = {3},
pages = {431--437},
title = {{Missing data and parameters estimates in multidimensional item response models}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=20705948{\&}AN=91706273{\&}h=rsRVTRu/xQ9uBKmfiz0Eu/yG/F7gm7I5I528dHCZF93HCQLaOApy8TyXEuiiLti0npIS8g3qLCx8PIEdbKK0XA=={\&}crl=c},
volume = {5},
year = {2012}
}
@book{Kalbfleisch2002,
author = {Kalbfleisch, John and Prentice, Ross},
booktitle = {Technometrics},
file = {:C$\backslash$:/Users/Nathan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalbfleisch, Prentice - 2002 - The Statistical Analysis of Failure Time Data.pdf:pdf},
isbn = {9780471363576},
title = {{The Statistical Analysis of Failure Time Data}},
year = {2002}
}
@article{Tsiatis2002,
author = {Tsiatis, Anastasios a. and Davidian, Marie and McNeney, Brad},
doi = {10.1093/biomet/89.1.238},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/Biometrika-2002-Tsiatis-238-44.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
number = {1},
pages = {238--244},
title = {{Multiple imputation methods for testing treatment differences in survival distributions with missing cause of failure}},
volume = {89},
year = {2002}
}
@article{Morisot2015a,
author = {Morisot, Adeline and Bessaoud, Fa{\"{\i}}za and Landais, Paul and R{\'{e}}billard, Xavier and Tr{\'{e}}tarre, Brigitte and Daur{\`{e}}s, Jean-Pierre},
doi = {10.1186/s12874-015-0048-4},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/s12874-015-0048-4.pdf:pdf},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {Multiple imputation,Net survival,Cause-specific su,cause-specific survival,erspc,multiple imputation,net survival},
number = {1},
pages = {54},
publisher = {BMC Medical Research Methodology},
title = {{Prostate cancer: net survival and cause-specific survival rates after multiple imputation}},
url = {http://www.biomedcentral.com/1471-2288/15/54},
volume = {15},
year = {2015}
}
@article{Hayes2013,
author = {Hayes, John R and Groner, Jonathan I},
doi = {10.1016/j.jpedsurg.2007.12.043.Using},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Hayes, Groner - Using multiple imputation and propensity scores to test the effect of car seats and seat belt usage on injury severity f.pdf:pdf},
journal = {Journal of Pediatric Surgery},
keywords = {and the,control interventions,events,including the inability to,multiple imputation,passenger safety,propensity score,prospective trials in pediatric,the unpredictability of trauma,there are several obstacles,to performing randomized,trauma,trauma registry},
number = {5},
pages = {924--927},
title = {{Using multiple imputation and propensity scores to test the effect of car seats and seat belt usage on injury severity from trauma registry data}},
volume = {43},
year = {2013}
}
@article{VanBuuren2011,
abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range ofmodels under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
author = {{Van Buuren}, Stef and Groothuis-Oudshoorn, Karin},
doi = {10.1177/0962280206074463},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/v45i03.pdf:pdf},
isbn = {9067436771},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {chained equations,fully conditional specification,gibbs sampler,mice,multiple imputation,passive imputation,predictor selection,r},
number = {3},
pages = {1--67},
pmid = {22289957},
title = {{mice: Multivariate Imputation by Chained Equations in R}},
url = {http://igitur-archive.library.uu.nl/fss/2010-0608-200146/UUindex.html},
volume = {45},
year = {2011}
}
@misc{Rubin1996,
abstract = {Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies.},
author = {Rubin, Donald},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1996.10476908},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Rubin - Multiple imputation after 18 years - 1996.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
number = {434},
pages = {473--489},
title = {{Multiple imputation after 18+ years}},
volume = {94},
year = {1996}
}
@article{Davidian2007,
author = {Davidian, Marie},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/davidian causal notes.pdf:pdf},
journal = {Statistics in Medicine},
number = {2004},
pages = {2937--2960},
title = {{Double Robustness in Estimation of Causal Treatment Effects}},
year = {2007}
}
@article{Vaughan2015a,
author = {Vaughan, Adam S and Kelley, Colleen F and Luisi, Nicole and del Rio, Carlos and Sullivan, Patrick S and Rosenberg, Eli S},
doi = {10.1186/s12874-015-0017-y},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/s12874-015-0017-y.pdf:pdf},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {STI,HIV,Propensity scores,Survival analysis,Men wh,asvaugh,correspondence,edu,emory,hiv,marginal structural models,men who have sex,propensity scores,sti,survival analysis,with men},
number = {1},
pages = {1--9},
title = {{An application of propensity score weighting to quantify the causal effect of rectal sexually transmitted infections on incident HIV among men who have sex with men}},
url = {http://www.biomedcentral.com/1471-2288/15/25},
volume = {15},
year = {2015}
}
@article{Zhu2015,
author = {Zhu, Hong and Lu, Bo},
doi = {10.1016/j.csda.2015.01.001},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/1-s2.0-S0167947315000110-main.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
pages = {42--51},
title = {{Multiple comparisons for survival data with propensity score adjustment}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947315000110},
volume = {86},
year = {2015}
}
@article{Rosenbaum1983,
annote = {Central Rubin paper about propensity score},
author = {Rosenbaum, Paul and Rubin, Donald},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Rosenbaum, Rubin - The central role of the propensity score in observational studies for causal effects - 1983.pdf:pdf},
number = {1},
pages = {41--55},
title = {{The Central Role of the Propensity Score in Observational Studies for Causal Effects}},
volume = {70},
year = {1983}
}
@book{Enders2010,
address = {New York},
author = {Enders, Craig K.},
booktitle = {Guilford Press},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/[Craig{\_}K.{\_}Enders{\_}PhD]{\_}Applied{\_}Missing{\_}Data{\_}Analysi(BookZZ.org).pdf:pdf},
isbn = {9781606236390},
pages = {401},
title = {{Applied Missing Data Analysis}},
year = {2010}
}
@article{Lunceford2004,
abstract = {Estimation of treatment effects with causal interpretation from observational data is complicated because exposure to treatment may be confounded with subject characteristics. The propensity score, the probability of treatment exposure conditional on covariates, is the basis for two approaches to adjusting for confounding: methods based on stratification of observations by quantiles of estimated propensity scores and methods based on weighting observations by the inverse of estimated propensity scores. We review popular versions of these approaches and related methods offering improved precision, describe theoretical properties and highlight their implications for practice, and present extensive comparisons of performance that provide guidance for practical use.},
author = {Lunceford, Jared K. and Davidian, Marie},
doi = {10.1002/sim.1903},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/statinmed.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Covariate balance,Double robustness,Inverse-probability-of-treatment-weighted-estimato,Observational data},
number = {19},
pages = {2937--2960},
pmid = {15351954},
title = {{Stratification and weighting via the propensity score in estimation of causal treatment effects: A comparative study}},
volume = {23},
year = {2004}
}
@article{Submitted,
author = {Submitted, A Thesis},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Submitted - Using multiple imputation , survival analysis , and propensity score analysis in cancer data with a large amount of missing.pdf:pdf},
title = {{Using multiple imputation , survival analysis , and propensity score analysis in cancer data with a large amount of missing data Requirements for the Degree Master of Arts Statistics}}
}
@article{Gheyas2009,
abstract = {The treatment of incomplete data is an important step in pre-processing data prior to later analysis. We propose a novel non-parametric multiple imputation algorithm for estimating missing value. The proposed algorithm is based on Generalized Regression Neural Networks. We compare the proposed algorithm against existing algorithms on forty-five real and synthetic datasets. The effectiveness of imputation algorithms is evaluated in classification problems. The performance of proposed algorithm appears to be superior to that of other algorithms.},
author = {Gheyas, Iffat a and Smith, Leslie S},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/WCE2009{\_}pp1281-1286.pdf:pdf},
isbn = {9789881821010},
journal = {Proceedings of the World Congress on Engineering},
keywords = {Missing values,imputation,multiple imputation,single imputation},
number = {0},
pages = {1--6},
title = {{A novel nonparametric multiple imputation algorithm for estimating missing data}},
volume = {II},
year = {2009}
}
@article{Gelman1992,
author = {Gelman, A. and Rubin, Donald},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Gelman, Rubin - Inference from iterative simulation using multiple sequences - 1992.pdf:pdf},
journal = {Statistical Science},
number = {4},
pages = {457--511},
title = {{Inference from iterative simulation using multiple sequences}},
volume = {7},
year = {1992}
}
@article{Hill2004a,
annote = {http://stats.stackexchange.com/questions/140761/survival-analysis-multiply-impute-5-datasets-to-average-one-propensity-score-th},
author = {Hill, Jennifer},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Hill - REDUCING BIAS IN TREATMENT EFFECT ESTIMATION IN OBSERVATIONAL STUDIES SUFFERING FROM MISSING DATA - 2004.pdf:pdf},
number = {January},
title = {{REDUCING BIAS IN TREATMENT EFFECT ESTIMATION IN OBSERVATIONAL STUDIES SUFFERING FROM MISSING DATA}},
year = {2004}
}
@article{cat2015,
author = {Tusell, Fernando},
file = {:C$\backslash$:/Users/Nathan/Documents/Mendeley Desktop/Tusell - Package ‘ cat ’ - 2015.pdf:pdf},
journal = {CRAN},
pages = {23},
title = {{Package ‘ cat ’}},
year = {2015}
}
@article{Harder2011,
author = {Harder, Valerie S. and Stuart, Elizabeth A. and {James C. Anthony}},
doi = {10.1037/a0019623.Propensity},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/nihms-192966.pdf:pdf},
journal = {Psychological Methods},
keywords = {computer-aided drug design,cyclophilin,free energy perturbation,hiv,reverse transcriptase},
number = {3},
pages = {234--249},
title = {{Propensity score techniques and the assessment of measured covariate balance to test causal associations in psychological research}},
volume = {15},
year = {2011}
}
@article{Hughes2014a,
abstract = {BACKGROUND: Chained equations imputation is widely used in medical research. It uses a set of conditional models, so is more flexible than joint modelling imputation for the imputation of different types of variables (e.g. binary, ordinal or unordered categorical). However, chained equations imputation does not correspond to drawing from a joint distribution when the conditional models are incompatible. Concurrently with our work, other authors have shown the equivalence of the two imputation methods in finite samples.$\backslash$n$\backslash$nMETHODS: Taking a different approach, we prove, in finite samples, sufficient conditions for chained equations and joint modelling to yield imputations from the same predictive distribution. Further, we apply this proof in four specific cases and conduct a simulation study which explores the consequences when the conditional models are compatible but the conditions otherwise are not satisfied.$\backslash$n$\backslash$nRESULTS: We provide an additional "non-informative margins" condition which, together with compatibility, is sufficient. We show that the non-informative margins condition is not satisfied, despite compatible conditional models, in a situation as simple as two continuous variables and one binary variable. Our simulation study demonstrates that as a consequence of this violation order effects can occur; that is, systematic differences depending upon the ordering of the variables in the chained equations algorithm. However, the order effects appear to be small, especially when associations between variables are weak.$\backslash$n$\backslash$nCONCLUSIONS: Since chained equations is typically used in medical research for datasets with different types of variables, researchers must be aware that order effects are likely to be ubiquitous, but our results suggest they may be small enough to be negligible.},
author = {Hughes, Rachael and White, Ian and Seaman, Shaun and Carpenter, James and Tilling, Kate and Sterne, Jonathan},
doi = {10.1186/1471-2288-14-28},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/1471-2288-14-28.pdf:pdf},
isbn = {1471-2288},
issn = {1471-2288},
journal = {BMC medical research methodology},
keywords = {chained equations imputation,gibbs sampling,joint modelling imputation,multiple imputation,multivariate missing data},
pages = {28},
pmid = {24559129},
title = {{Joint modelling rationale for chained equations.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3936896{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {14},
year = {2014}
}
@article{Olmos2015,
abstract = {Propensity score weighting is one of the techniques used in controlling for selection biases in nonexperimental studies. Propensity scores can be used as weights to account for selection assignment differences between treatment and comparison groups. One of the advantages of this approach is that all the individuals in the study can be used for the outcomes evaluation. In this paper, we demonstrate how to conduct propensity score weighting using R. The purpose is to provide a stepby- step guide to propensity score weighting implementation for practitioners. In addition to strengths, some limitations of propensity score weighting are discussed.},
author = {Olmos, Antonio and Govindasamy, Priyalatha},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/getvn.pdf:pdf},
journal = {Practical Assessment, Research {\&} Evaluation},
number = {13},
pages = {1--8},
title = {{A Practical Guide for Using Propensity Score Weighting in R}},
url = {http://pareonline.net/getvn.asp?v=20{\&}n=13},
volume = {20},
year = {2015}
}
@article{VanBuuren2006,
abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
annote = {Simulation study that shows why compatability isn't really an issue},
author = {{Van Buuren}, Stef and Brand, J. P.L. and Groothuis-Oudshoorn, C. G.M. and Rubin, Donald},
doi = {10.1080/10629360600810434},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/10629360600810434.pdf:pdf},
isbn = {0094-9655},
issn = {0094-9655},
journal = {Journal of Statistical Computation and Simulation},
number = {12},
pages = {1049--1064},
title = {{Fully conditional specification in multivariate imputation}},
volume = {76},
year = {2006}
}
@book{Klein1984,
author = {Klein, John and Moeschberger, Melvin},
doi = {10.1145/390011.808243},
file = {:C$\backslash$:/Users/Nathan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein, Moeschberger - 1984 - Techniques for Censored and Truncated Data.pdf:pdf},
isbn = {038795399X},
issn = {03621340},
pages = {536},
title = {{Techniques for Censored and Truncated Data}},
year = {1984}
}
@article{Schoenfeld1982,
author = {Schoenfeld, David},
doi = {Doi 10.2307/2335876},
file = {:C$\backslash$:/Users/Nathan/OneDrive/PDF books/239.full.pdf:pdf},
isbn = {0006-3444},
journal = {Biometrika},
number = {1},
pages = {239--241},
title = {{Partial Residuals for the Proportional Hazards Regression-Model}},
url = {<Go to ISI>://A1982NL69300029},
volume = {69},
year = {1982}
}
